{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation using real-valued computational graph \n",
    "\n",
    "```\n",
    " dQ/dX\n",
    "X ---\\\n",
    "      \\    df/dQ\n",
    "       (+)---\\\n",
    " dQ/dY/   Q   \\\n",
    "Y ---/         \\\n",
    "                (*)--- f(x, y, z)\n",
    "               /   df/df\n",
    " df/dZ        /\n",
    "Z -----------/\n",
    "```\n",
    "\n",
    "#### By Chain rule\n",
    "```\n",
    "given Q = X + Y\n",
    "\n",
    "df/dQ = df/df * df/dQ\n",
    "df/dX = df/dQ * dQ/dX\n",
    "df/dY = df/dQ * dQ/dY\n",
    "df/dZ = df/df * df/dZ\n",
    "```\n",
    "by manually calculating the partial derivatives...\n",
    "```\n",
    "df/dQ = Z\n",
    "df/dX = Z * 1\n",
    "df/dY = Z * 1\n",
    "df/dZ = X + Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, y, z):\n",
    "    return z*(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(x, y, z):\n",
    "    dfdx = z\n",
    "    dfdy = z\n",
    "    dfdz = x + y\n",
    "    return dfdx, dfdy, dfdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients on X: 9, Y: 9 and Z: 7\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 5\n",
    "z = 9\n",
    "dx, dy, dz = backward(x, y, z)\n",
    "print (\"Gradients on X: {}, Y: {} and Z: {}\".format(dx, dy, dz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Converting the above computation graph to an optimization problem\n",
    "\n",
    "### This can simply be done by adding a loss layer at the end. Lets say we add a mean square loss `node` after `f(x, y, z)`. Here we define value `V` that we want to optimize using the function `f(x, y, z)`. Below is how the new computational graph would look like. Notice that we are taking derivatives w.r.t `L` not `f` anymore.\n",
    "\n",
    "```\n",
    " dQ/dX\n",
    "X ---\\\n",
    "      \\    df/dQ\n",
    "       (+)---\\\n",
    " dQ/dY/   Q   \\\n",
    "Y ---/         \\\n",
    "                (*)--- f(x, y, z)---Loss\n",
    "               /   dL/df\n",
    "        df/dZ /\n",
    "Z -----------/\n",
    "```\n",
    "\n",
    "```\n",
    "L = (V - f(x,y,z))^2\n",
    "dL/dQ = dL/df * df/dQ\n",
    "dL/dX = dL/dQ * dQ/dX\n",
    "dL/dY = dL/dQ * dQ/dY\n",
    "dL/dZ = dL/df * df/dZ\n",
    "```\n",
    "\n",
    "analytically this becomes...\n",
    "\n",
    "```\n",
    "dL/dQ = -2(V-f)*Z\n",
    "dL/dX = -2(V-f)*Z*1 = dL/dQ*Z\n",
    "dL/dY = -2(V-f)*Z*1 = dL/dQ*Z\n",
    "dL/dZ = -2(V-f)*(X + Y)\n",
    "\n",
    "```\n",
    "\n",
    "now the backward calculation changes to below. You can verify this using wolframalpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_w_loss(V, x, y, z):\n",
    "    dldx = -2 * z * (V - (x + y) * z)\n",
    "    dldy = -2 * z * (V - (x + y) * z)\n",
    "    dldz = -2 * (x + y) * (V - (x + y) * z)\n",
    "    return dldx, dldy, dldz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization loop for variables `x`, `y` and `z`\n",
    "\n",
    "Here I am optimizing the variables to produce a `final_value` of `144`. Which is partly equivalent to training a neural network to learn a certain output/final_value. Notice how in the iteration, you do not need the `forward` function (only in this case because it is a simple/small network and we are solving the entire network analytically). If you were do this piecewise as in a `real` neural network, you need to do the `forward` pass to calculate the loss, which will then get backpropogated through the network.\n",
    "\n",
    "Play around with all initial values of the variables (long enough) and you will essentially find all the real values of `x`, `y` and `z` that satisfy the equation. \n",
    "\n",
    "Notice that this problem has the same steps as the previous curvefitting assigment. Here instead you are training a computational graph to approximate an output value. Also a more complex computational graph like a neural network will follow the same steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Values of x: 13.1042097033, y: 2.20330970331 and z: 9.40714142997\n",
      "Evaluation of (x + y) * z = 144.0\n"
     ]
    }
   ],
   "source": [
    "x = 12.3791\n",
    "y = 1.4782\n",
    "z = 8.192\n",
    "alpha = 0.001\n",
    "final_value = 144\n",
    "n = 1000\n",
    "for i in range(n):\n",
    "    dx, dy, dz = backward_w_loss(final_value, x, y, z)\n",
    "    x = x - alpha * dx\n",
    "    y = y - alpha * dy\n",
    "    z = z - alpha * dz\n",
    "print (\"Final Values of x: {}, y: {} and z: {}\".format(x, y, z))\n",
    "# forward is only used here below\n",
    "print (\"Evaluation of (x + y) * z = {}\".format(forward(x, y, z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackPropogation on different and much complex computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    " \n",
    "X1 ---\\\n",
    "       \\\n",
    "       (*)----\\\n",
    "       /       \\\n",
    "W1 ---/         \\\n",
    "                 \\\n",
    "                 (-)-----\\\n",
    "                 /        \\  \n",
    "X2 ----\\        /          \\\n",
    "        \\      /            \\\n",
    "        (*)---/             (+)------ tanh(O) ----- Loss\n",
    "        /                   /\n",
    "W2 ----/                   /    \n",
    "                          /\n",
    "                         /\n",
    "X3 ---------------------/\n",
    "\n",
    "\n",
    "X1, X2, X3 are inputs\n",
    "W1, W2 are weights \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent non-linearity\n",
    "$$\\tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\delta \\tanh(x)}{\\delta x} = \\frac{4 e^{2x}}{(e^{2x}+1)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base object for all inputs and outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, value, grad):\n",
    "        self.value = value\n",
    "        self.gradient = grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyNode(object):\n",
    "    \"\"\"\n",
    "    Multiplies two inputs\n",
    "    \"\"\"\n",
    "    def forward(self, x1, x2):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.output = Node(self.x1.value * self.x2.value, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        self.x1.gradient = self.x2.value * self.output.gradient\n",
    "        self.x2.gradient = self.x1.value * self.output.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNode(object):    \n",
    "    \"\"\"\n",
    "    Adds two inputs x1 and x2.\n",
    "    \"\"\"\n",
    "    def forward(self, x1, x2):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.output = Node(self.x1.value + self.x2.value, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        self.x1.gradient = 1 * self.output.gradient\n",
    "        self.x2.gradient = 1 * self.output.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtractNode(object):    \n",
    "    \"\"\"\n",
    "    subtract two inputs x1 and x2.\n",
    "    \"\"\"\n",
    "    def forward(self, x1, x2):\n",
    "        self.x1 = x1\n",
    "        self.x2 = x2\n",
    "        self.output = Node(self.x1.value - self.x2.value, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        self.x1.gradient = 1 * self.output.gradient\n",
    "        self.x2.gradient = -1 * self.output.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperbolicTangentNode(object):\n",
    "    \"\"\"\n",
    "    Adds a Hyperbolic Tangent non-linearity to a single input\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.output = Node(((np.exp(2 * self.x.value) - 1)/ (np.exp(2 * self.x.value) + 1)), 0.0)\n",
    "        return self.output\n",
    "        \n",
    "    def backward(self):\n",
    "        t = (4 * (np.exp(2 * self.x.value)))/ ((np.exp(2 * self.x.value) + 1) ** 2)\n",
    "        self.x.gradient = t * self.output.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_nn():\n",
    "    # w1 * x1\n",
    "    w1x1 = w1_mul_x1.forward(w1, x1)\n",
    "    # w2 * x2\n",
    "    w2x2 = w2_mul_x2.forward(w2, x2)\n",
    "    # w1*x1 - w2*x2\n",
    "    w1x1_w2x2 = w1x1_subtract_w2x2.forward(w1x1, w2x2)\n",
    "    # w1*x1 - w2*x2 + x3\n",
    "    w1x1_w2x2_x3 = w1x1w2x2_add_x3.forward(w1x1_w2x2, x3)\n",
    "    # HyperbolicTangent(w1*x1 - w2*x2 + x3)\n",
    "    output = HyperbolicTangent_out.forward(w1x1_w2x2_x3)\n",
    "    return output\n",
    "\n",
    "def backward_nn():\n",
    "    HyperbolicTangent_out.backward()\n",
    "    w1x1w2x2_add_x3.backward()\n",
    "    w1x1_subtract_w2x2.backward()\n",
    "    w2_mul_x2.backward()\n",
    "    w1_mul_x1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weights and Bias\n",
    "w1 = Node(0.1, 0.0)\n",
    "w2 = Node(0.4, 0.0)\n",
    "\n",
    "# Input/Target Output\n",
    "alpha = 0.001\n",
    "x1 = Node(1.0, 0.0)\n",
    "x2 = Node(1.0, 0.0)\n",
    "x3 = Node(-0.02, 0.0)\n",
    "final_value = 0.475\n",
    "\n",
    "# Create Nodes\n",
    "w1_mul_x1 = MultiplyNode()\n",
    "w2_mul_x2 = MultiplyNode()\n",
    "w1x1_subtract_w2x2 = SubtractNode()\n",
    "w1x1w2x2_add_x3 = AddNode()\n",
    "HyperbolicTangent_out = HyperbolicTangentNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47499999999999887"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1000000):\n",
    "    forward_output = forward_nn()\n",
    "    forward_output.gradient = -2 * (final_value - forward_output.value)\n",
    "    backward_nn()\n",
    "    w1.value -= alpha * w1.gradient\n",
    "    w2.value -= alpha * w2.gradient\n",
    "    \n",
    "forward_output.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
